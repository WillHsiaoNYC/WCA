# WCA Data Pipeline

This project automatically:
1. **Downloads** the WCA RanksSingle dataset (if not already present).
2. **Scrapes** a specified WCA competition’s registrations page (using Selenium).
3. **Matches** each scraped WCA ID against the WCA dataset.
4. **Pivots** each competitor’s records (by event) into columns for easy analysis.
5. **Formats** “best” columns by dividing by 100 and rounding to 1 decimal place.
6. **Sorts** all rows by `333_worldRank` (placing missing values at the end).
7. **Outputs** the final dataset to a CSV file with columns reordered such that:
   - “Name” is the first column.
   - “personId” is the second column.
   - All other columns are sorted alphabetically.

---

## Table of Contents
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Configuration](#configuration)
- [Limitations & Notes](#limitations--notes)
- [License](#license)

---

## Prerequisites

1. **Python 3.7+**  
   Make sure Python is installed on your system.  
   [Download Python](https://www.python.org/downloads/)

2. **Chrome Browser**  
   The script uses Selenium to automate Google Chrome. Install the latest [Google Chrome](https://www.google.com/chrome/).

3. **ChromeDriver**  
   - Download the **ChromeDriver** matching your exact Chrome version:  
     [ChromeDriver Downloads](https://chromedriver.chromium.org/downloads)
   - Place `chromedriver` in your [system PATH](https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/) so Selenium can find it.

---

## Installation

1. **Clone** this repository (or download the ZIP):
   ```bash
   git clone https://github.com/<your-username>/<your-repo>.git
   cd <your-repo>
   ```

2. **Install Python Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
   Make sure your `requirements.txt` contains:
   ```
   requests
   pandas
   selenium
   ```

3. **(Optional) Create a Virtual Environment**  
   It’s recommended to run the script in a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On macOS/Linux
   venv\Scripts\activate     # On Windows
   pip install -r requirements.txt
   ```

---

## Usage

1. **Open a Terminal** in the project folder.
2. **Run the Python script**:
   ```bash
   python main.py
   ```
3. The script will:
   - Check if `WCA_export_RanksSingle.tsv` already exists; if not, download and extract it.
   - Scrape the registrations page for the competition set in the code (e.g., `GreatPeconicBay2025`).
   - Match each competitor’s WCA ID against the dataset.
   - Pivot the results, sort columns, format “best” values, and sort rows by `333_worldRank`.
   - Output the final CSV file as `competitor_rankings.csv`.

---

## Project Structure

```
.
├── main.py
├── requirements.txt
├── WCA_export_RanksSingle.tsv  (optional, auto-downloaded if absent)
├── competitor_rankings.csv     (generated by the script)
└── README.md
```

---

## License

[MIT License](LICENSE)  
Feel free to use, modify, and distribute this project. Please acknowledge the [World Cube Association (WCA)](https://www.worldcubeassociation.org/) for the data source.
